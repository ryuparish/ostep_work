--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The Abstraction, The Process

1. Both of them should run completely, and the cpu should be busy 100% of the time. If that were not the case, the CPU would be used in an inefficient way.

2. The first process only takes 4 time because the cpu doesn't have to wait on anything. The second process however, need to do I/O and the CPU is not used, the kernel just keeps track of the process then recieves an interrupt to detect it is done.

3. The ordering of the scheduler matters because a program can perform I/O simultaneously while the CPU is running another program, thus more efficient usage if you schedule the I/O first.

4. When the SWITCH_ON_END flag is set, the programs take almost double the time than without the flag. The CPU and I/O are not efficiently used. 

5. The same time and situation for processes without the switch occurs (efficient usage)

6. No, the systems resources are not being efficiently utilized because the processes should be running parallel with the I/O. In this case, the first I/O occurs, then all the processes are run before the next 3 I/O's occur (these two things can be done in tandem).

7. As described in my previous answer, now the CPU is being used 100% of the time and the I/O is being completed in tandem. This reduces the time by about 30% and is more efficient.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

CPU Scheduler

4. When the jobs are given in Increasing order all at the same time (more like with a neglible amount of time in between) the FIFO policy and SJF policy will both run at the same speeds

5. The RR and SJF times would be the same if you set the quantum to 0 and then input the jobs in Increasing order to the RR policy. It would also be the same if you had all the jobs of the same length and set the quantum to be that length.

6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times6. As the job lengths increase equally among all the programs, the shortest job can have a large amount of time, meaning the jobs behind the first will get longer and longer response times

7. The response time becomes longer and longer. The equation for the longest response time would be the n + n + n + ... / 1 ( which is the same thing a FIFO )

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. "Long Running Job Over Time" - The scheduler would have a constant quanta across all three priority queues, no boosts, and just one CPU-hogging job.

"Along Came An Interactive Job" - Have a schduler that still will lower jobs that take their entire quantum, but no more complexity other than that and the queue priorities themselves. After a CPU hogging process sinks and stays at the bottom for a while, a shorter program comes in and finishes first. There is no boosting, so the second program completes in its entirety then the first cpu hog resumes.

"A Mixed I/O intensive and CPU intensive Workload" - Have the scheduler run basic with a lower in priority if the program runs for the whole time (no boost) and let the ones at the bottome run while the I/O intensive programs run their I/O. The CPU and I/O intensive programs run in an ideal way (although it is only two programs).

"Priority Boost" - Every few miliseconds on an interval reset the priority of all the jobs to allow at least some progress to be getting done on the CPU intensive-long-time program (avoiding starvation). Still running Round Robin for the two upper programs and letting the lower CPU intense program just run a single interval every time the boost comes in.

"Gaming Tolerence" - The problem that is aimed to be avoided is a job that may game the system by stopping short by the smallest amount of detectable time so that the priority of the job never drops, even if it uses 99% the same amount of cpu that the "no I/O" CPU intensive program uses. To avoid this, we not only take into account the number of times a job conducts I/O, but we also take into account how much total time is takes out of the alotted time given. This will prevent the gaming of the system.



"Lower Priority, Longer Quanta" - Assuming that the lower quanta is only relevant when there are no upper priority jobs, I think this method would be done by allowing different Quanta for each of the priority levels, but then you would only need the lower priority levels if there were no upper priority level jobs. If there are upper priority level jobs, then the lower level CPU intensive jobs would only be able to run if they were boosted or during I/O.

3. I would just set the queues to be 1, the quantum to be a set amount and (if possible) just let the CPU run idle if there is I/O for the currently worked job.

4.  python3 mlfq.py -n 3 -Q 10,1,1 -l 0,50,9:0,50,0 -c -i 1 -a 1 -S

5. This would depend on how many jobs there were and how long they would run (if they ran i/o or not). 

6. If there was a job that would run i/o right before the end of the quanta and the length of the i/o was 1, then it would makes sense to just let the job run once the i/o finishes to reduce the turnaround time of the job (or multiple if there are many that do that).


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Segmentation Chapter 16

1. a) 0: PA 492 - Since this has a highbit value of 0, this will on the stack (from 512 to 492 in physical memory). The length of the segment will be Virtual_address_size - Virtual address (128 - 108 = 20 bytes)

    So this program will completely take up the stack, but will still fit into 512 - 492.
      1 & the rest: The rest are all less that 108 and are on the stack, therefore the rest of them will not fit in the physical memory.

   b) 0: PA 17 - Okay I guess there is no way to tell if these points are in the heap or the stack because I don't see any bits that say anything. This one would fit on the heap since it is less than 20.
      1: PA 492 - This one would fit on the stack since 128 - 108 < 20.
      2, 3, 4: SEGFAULT - This one does not work for either the stack or the heap. 

   c) 0: PA 506 (Stack)
      1: PA 505 (Stack)
      2: PA 7 (Heap)
      3: PA 10 (Heap)
      4: SEGFAULT

2. a) The highest legal virtual address in the previous question is 19 in segment 0.

   b) The lowest legal virtual address segment is seg 1 is 108.

   c) The lowest illegal address space is 21 and the highest illegal address space is 107.

   d) python3 segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2 -A 19,108,21,107 -c

3. segmentation.py -a 16 -p 128 -A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 --b0 0 --l0 2 --b1 128 --l1 2

4. You should set the combined total of the heap physical space and the stack physical space to be 10% of the virtual memory, therefor if you randomly generate, only 10% should end up being valid.

5. Make the heap space larger or smaller than max or minimum value of the virtual addresses and then make the physical stack space smaller than all the differences between the address space size minus the virtual addresses.


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Malloc.py

5. The ordering matters in this case because if the list is ordered in a way such as sizesort, the chances that the addresses will be next to each other for coalescing is small. Therefore, no coalescing would happen and external fragementation would flourish.

6. The higher the percentage you allocate without giving back, the less of a chance there is for a neighboring address to be deallocated. Therefore, the chances of coalescing lessen and the memory becomes more fragmented.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Small Table paging

1. Instead of only a base register for the page table(linear), you would need a page table for the page directory(1) then you would need to keep track of each page-directory-page register(n), since the physical location where the pages are stored may be scattered or just in different places.


